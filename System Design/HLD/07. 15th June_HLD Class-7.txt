Date : 15th June 2024
Mentor: DEVANG SHARMA
Batch: May Batch (System Design) - Tutort Academy

Agenda:  Basics of System Design

- Scalability: DONE
- Availability: DONE
- Consistency: DONE
- Load Balancing: WIP
- Reliability
- Efficiency
- Manageability
- Resiliency
- Load Balancing
- Caching: DONE
- Choice of DB: DONE
- CAP Theorem: DONE
- Polling (Long/Short) vs Web Sockets vs SSE: DONE
- Data Partitioning: DONE
- Indexing: DONE
- Redundancy and Replication: DONE
- Consistent Hashing
- Proxies and Gateways

(1) System Design Interviews: A step by step guide: DONE
(2) Designing a URL Shortening service like TinyURL: DONE
(3) Designing Pastebin: WIP
(4) Designing Instagram

"Please Type 'Hi' in the Chat Box if you have joined and Can See this Screen".




(8) Data Partitioning and Replication


To scale out our DB, we need to partition it so that we can store information
about Billions of URLs

--------> What Should be the Partitioning Schemes?

(1) Range Based Partitioning

Short URLs: 6-8 Characters

- We can store URLs in separate partitioning  based on the first letter of hash key

- Save All URLs starting with letter 'a' - partition: 1
- Save All URLs starting with letter 'b' - partition: 2

a-z: 26
A-Z: 26
0-9: 10
Total: 62

Total Partitions: 62


Problem:

- Unbalanced Partitioning/ Uneven Load Distribution


Eg:

partition-1: starting with 'a' has more Short URLs: HIGH LOAD
partition-2: starting with 'b' has less Short URLs: LESS LOAD




(2) Hash Based Partitioning

We take hash of object we are storing
Then calculate which partition to use based upon hash


In Our Use Case:

Short URL ------> Hash ------> Key
Based upon Key -----> Partition Number



Eg:

hash() = key % 256 (any constant)

Hash Upto 256 Values


256 Partitions -----> Follow Round Robin Approach


req-1 - 1st short URL: DB Partition-1
req-1 - 2nd short URL: DB Partition-2
.....................................







Replication of Partitions:

To Avoid any point of failure,
We can replicate each partitions






(9) Caching:

- Which Cache (Why?)
- Size of Cache (80-20 Rule)
- Which Cache Eviction Policy? (Why)
- Handle Cache Invalidation (Cache Miss Case)


-----> Which Cache

Cache:
Cache the URLs that are frequently used

Eg: Redis, Memcache, Hazelcast


The Application Servers, before hitting the DB Storage ------> Quickly check in Cache



-------> Size of Cache



Following 80-20 Rule for My Cache:
Cache 20% of my data


To Store 20% of these Requests:

0.2 * 1.7 Billion * 500 Bytes/URL = 170 GB of Cache Memory per Day



Server: 256 GB/ 512 GB
1 Server is sufficient




------> Which Cache Eviction Policy? 


When the cache is full, and we want to replace a link with a newer link

link-1: 5 Times: Last Used: 24 Hrs Ago - LFU
link-2: 10 Times: Last Used: 1 Year Ago - LRU


LRU can be a good solution.
Discard the least recently used URL first and store the new URL






---------> How can each replica of cache be updated?

In Case of Cache Miss,
Request will go to Database


When this happens,
After Retrieving from Database, we can update the Cache and pass the new entry to all replicas.


- Each replica can update its cache by adding the new entry
- If a replica already has that entry, ignire it.




Image: 
HLD Diagram





(10) Load Balancers

Where do we Add Load Balancers:

(1) Between Client and Application Servers
(2) Between Application Servers and Database Servers
(3) Between Application Servers and Cache Servers



------> Which Policy to use for Load Balancers?

(1) Use Round Robin:

Adv:

- Equally distributed load among servers
- If 1 server is not responding, LB will move to next available server

(2) Problem with Round Robin?
- Load amount is not taken care of for a server


Better Solution:

Predictive Analysis for Load Balancing/Smart Load Balancers

LB Solution can be periodically queries to know the health and load of DB Servers

Eg:

Netapp,
NginX






(11) Purging or DB Cleanup Service

Do I need URL Entries Forever?
- No


Should I check daily or actively for expired links?
- Put a lot of pressure on servers


Instead,
We can slowly remove expired links and do a lazy cleanup.
Our Cleanup Service will make sure that only expired links are deleted


Possible Scenarios:

(1) When a user tries to access an expired link

google.com -------> tinyurl/abc

After 5 Years, 
tinyurl/abc has expired 

(1) delete the short url key
(2) Mark that short url key -> available to use
(3) Reponse to Client: URL expired


(2) A separate cleanup service can run periodically to remove the expired link
from DB and Cache.
Serices should be VERY Lighweight (less resource intensive)


Run Only when the user-traffic is at non-peak hours.


(3) Default Expiration Time: 5 Years

(4) After removing a short URL Key, 
We can put back that key in Key-DB to be reused.


Non-Peak Hours:

Grafana, Kibana, Dynatrace, AWS Metrics







(12) Analytics/Telemetry: EXTENDED

Metrics:

- Country of Visitors
- Date and Time
- No Views
- CTR etc



Generate Kafka Event everytime a URL Re-direction happens
Put that Event in an EQ (Event Queue) for Analytics

On Top of that, 
Run Apache Spark and Hadoop Queries to get more insights.


(13) Security and Permissions: EXTENDED


Can Users create a custom shortened URL?


Can Users create a private shortened URL for particular set of users ONLY?


Solution:

You can use permssion levels (public/private) of URL 
along with URL in Database

private: Specific Set of Users ----> Whitelist them


For Custom Shortened URL,
Check the (isPremiumMember == true).


































--------> Case Study-2: Designing a Text Sharing Service like Pastebin

Pastebin = TinyUrl (URL Shortening) + Data

Pastebin:

- Web Service where users can store textual content (plain text, Software Licenses, Source Code etc)



Similar Services:
pasted.co, chopapp.com



(1) Demo:
https://pastebin.com


Eg:
https://pastebin.com/gr8gLtJG


- Unique Link for Every Paste
- Link can be Public/Unlisted/Private
- API/Tools Integration
- Analytics/ Telemetry


Allows Users to store textual content and generate a short Unique URL for it.



(2) Requirements Clarification:

(A) Functional Requirements
- User should be able to upload or paste their textual data and get a unique URL
to access it.
- Upload: Only Textual Data (.txt. .json etc)
- Only able to upload textual data
- Each data and link will have a default timespan before it expires (5 Years)
- Provides Option to User to Delete the Paste
- Shortened Pastebin URL should be Unique
- Max Content Size: 10K Chars
- Formatting, Languages Support, Syntax Highlighting
- Author can update the edit (After Sign In)



Additional:
- User should be able to pick a custim short link fir URL
- User can explicitly mention expiry time




(B) Non-Functional Requirements

- System should be HIGHLY Available
Because, If Our System is Down, All Pastebin URL will Start Failing
- System should be HIGHLY RELIABLE
Any data uploaded/pasted should not be lost.
- Users should be able to access their Paste/Content in Real Time (Minimum Latency)


(C) Extended Requirements:
- Analytics/ Telemetry: Clicks, Views, Audience etc
- Third Party Integrations: REST API to be consumed by Other Systems







Very Popular Services:
- Very Popular Services: Instagram/FB/Twitter/Uber etc: 1 Bn+ Requests/Month
- Medium Popular Services: TinyURL, Picaso, Figma etc: 500 Mn Requests/Month
- Less Popular Services: Pastebin, Imgur: 50 Mn Requests/Month



(3) Capacity Estimation/ Back of the Envelope Estimation


Read-Heavy or Write-Heavy?


- Read-Heavy
- Lots of Readsfrom Pastebin as compared to new Writes
- Read-Write Ratio: 10:1


(A) Traffic Estimates:

Assumption,
1 Million paste to our system on daily basis.

- Read-Write Ratio: 10:1

No of Write Calls in a Second = 1 Mn/24 Hrs * 3600 sec ~= 12 pastes/sec

No of Read Calls in a Second = 10:1 Ratio ~= 120 reads/sec


(B) Queries Per Second

------> Write Case:
No of Write Calls in a Second = 1 Mn/24 Hrs * 3600 sec ~= 12 pastes/sec



------> Read Case:
R/W Ratio: 10: 1


No of Read Calls in a Second = 10:1 Ratio ~= 120 reads/sec



(C) Storage Estimates

Assumption:
Every Paste is stored for 5 Years/ Expires after 5 Years

Limit: Users can upload maximum of 10 KB of data

Why?
- Data stored is: Source Code, Configs, Logs -  Textual Data


Average Storage Estimates per day:
1 Mn * 10 KB  ~= 10 GB/Day


Total Number of Links/Paste to Store:

30 Million URLs every Month * 12 Months * 5 Years = 1.8 Billions pastes


Content Size = 10 KB
Additional Details: user_uuid, location, creation_time, expiry_time, metadata etc

Total Size for 1 Paste = 10 KB + 10 KB = 20 KB

Total Storage Required in 5 Years
= 1.8 Billions Paste * 20 KB = 36 TB




(D) Cache Memory Estimates

120 Read Requests/second * 3600 * 24 = 10 Millions Read Requests/Day

Following 80-20 Rule for My Cache:
Cache 20% of my data


To Store 20% of these Requests:

0.2 * 10 Millions * 20 KB/URL = 400 GB of Cache Memory/Day






















